{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background on likelihood inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This document will serve as the mathematical background to Estimagic's likelihood inference functionality. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maximum-likelihood estimation seeks to maximize the likelihood function, defined as a function of an unknown parameter vector, **$\\beta$** , conditional on the data provided. For simplicity, we represent such a function as: $$L(\\beta|y_j, x_j)$$ where $y_j$ is the observed outcome variable for observation $j$ from the sample data and $x_j$ is the $j$th row of covariates for the same observation. Specifically, the function is called the **joint density** since the probability density function for each observation is seen as independent of one another and the distribution from which the observed outcome variable is generated stays the same for each observation [2]; thus, we seek to maximize the product of the densities $$\\prod_{j=1}^{n} f(y_j| x_j, \\beta) = L(\\beta|y_j, x_j) $$ In practice, the log of the likelihood function is used because it is simpler to maximize. The logarithm is a monotonic transformation, thus the parameters that maximize the product of the densities will also maximize the sum of the log-likelihoods. $$\\ln L(\\beta|y_j, x_j) = \\sum^n_{j=1} \\ln f(y_j|x_j, \\beta) $$ \n",
    "\n",
    "The functional form of $f$ depends on the distribution of the disturbance or outcome variable from the estimation model. For the present documentation, we consider the logit model and thus, a logistical distribution for our outcome variable. A logistical distribution is used to estimate outcomes where the probability of observing an outcome lies between 0 and 1 and the sum of the probabilities is equal to 1. Its log-functional form or log-cumulative density function is shown and replaces $f$ from the above expression: $$\\ln \\left(\\frac{1}{1+e^{-(2y_j-1)(x_j \\beta)}}\\right)\\longrightarrow \\sum^n_{j=1} \\ln \\left(\\frac{1}{1+e^{-(2y_j-1)(x_j \\beta)}}\\right)$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_likelihood_obs(params, y, x, criterion, design_options):\n",
    "    \"\"\"Pseudo-log-likelihood contribution per individual.\n",
    "\n",
    "    Args:\n",
    "        params (pd.DataFrame): The index consists of the parmater names,\n",
    "            the \"value\" column are the parameter values.\n",
    "        y (np.array): 1d numpy array with the dependent variable\n",
    "        x (np.array): 2d numpy array with the independent variables\n",
    "        criterion (string): \"logit\" or \"probit\"\n",
    "        design_options (pd.DataFrame): dataframe containing psu, stratum,\n",
    "            population/design weight and/or a finite population corrector (fpc)\n",
    "            \n",
    "    Returns:\n",
    "        loglike (np.array): 1d numpy array with likelihood contribution per individual\n",
    "\n",
    "    \"\"\"\n",
    "    q = 2 * y - 1\n",
    "    if criterion==\"logit\":\n",
    "        c = np.log(1 / (1 + np.exp(-(q * np.dot(x, params[\"value\"])))))\n",
    "        if \"weight\" in design_options.columns:\n",
    "            return c * design_options[\"weight\"].to_numpy()\n",
    "        else:\n",
    "            return c\n",
    "    elif criterion==\"probit\":\n",
    "        c = np.log(stats.norm._cdf(np.dot(q[:, None] * x, params[\"value\"])))\n",
    "        if \"weight\" in design_options.columns:\n",
    "            return c * design_options[\"weight\"].to_numpy()\n",
    "        else:\n",
    "            return c\n",
    "    else:\n",
    "        print(\"Criterion function is misspecified or not supported.\")\n",
    "\n",
    "def estimate_likelihood(params, y, x, criterion, design_options):\n",
    "    \"\"\"Pseudo-log-likelihood.\n",
    "\n",
    "    Args:\n",
    "        params (pd.DataFrame): The index consists of the parameter names,\n",
    "            the \"value\" column are the parameter values.\n",
    "        y (np.array): 1d numpy array with the dependent variable\n",
    "        x (np.array): 2d numpy array with the independent variables\n",
    "        criterion (string): \"logit\" or \"probit\"\n",
    "        design_options (pd.DataFrame): dataframe containing psu, stratum,\n",
    "            population/design weight and/or a finite population corrector (fpc)\n",
    "            \n",
    "    Returns:\n",
    "        loglike (np.array): 1d numpy array with sum of likelihood contributions\n",
    "\n",
    "    \"\"\"\n",
    "    return estimate_likelihood_obs(params, y, x, criterion, design_options).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the vector of parameters which maximize the log-likelihood function, we take first order conditions with respect to the parameter vector: $$G(\\beta) = \\sum^n_{j=1} \\frac{\\partial \\ln L(\\beta|y_j, x_j)}{\\partial \\beta} = 0$$ The partial derivative of the log-likelihood for all observations with respect to the vector of parameters makes the **score vector** and we express the sum over all scores as $G(\\beta)$. The existence of a global maximum depends on the shape of the function we are estimating. Search for the maximum if the log-likelihood function is concave. Search for the minimum if the log-likelihood is convex (which occurs if you flip the sign of the function). \n",
    "\n",
    "To generate the parameters which maximize the log-likelihood function, we use Estimagic's `maximize` functionality. For maximum-likelihood estimations, there is no closed-form solution; thus, numerical optimization is requisite in estimating the set of parameters. To estimate, there are several methods. We use the L-BFGS-B algorithm. Running the algorithm returns a set of parameters which maximize the log-likelihood. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_parameters(criterion, formulas, data, design_options, dashboard=False):\n",
    "    \"\"\"Estimate parameters that maximize log-likelihood function.\n",
    "\n",
    "    Args:\n",
    "        criterion (string): \"logit\" or \"probit\"\n",
    "        formulas (str or list of strings): a list of strings to be used by \n",
    "            patsy to extract dataframes of the dep. and ind. variables\n",
    "        data (pd.DataFrame): a pandas DataFrame\n",
    "        design_options (pd.DataFrame): dataframe containing psu, stratum,\n",
    "            population/design weight and/or a finite population corrector (fpc)\n",
    "        dashboard (bool): Switch on the dashboard\n",
    "\n",
    "    Returns:\n",
    "        res: parameter vector and other optimization results.\n",
    "\n",
    "    \"\"\"\n",
    "    params, y, x = mle_processing(formulas, data)\n",
    "\n",
    "    res = maximize(\n",
    "        criterion=estimate_likelihood,\n",
    "        params=params,\n",
    "        algorithm='scipy_L-BFGS-B',\n",
    "        criterion_kwargs={\"y\": y, \n",
    "                          \"x\": x, \n",
    "                          \"criterion\": criterion, \n",
    "                          \"design_options\": design_options\n",
    "                          },\n",
    "        dashboard=dashboard)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To extract the standard-errors, we first solve for the variance of the parameter vector. There are three methods to estimate the variance: \n",
    "\n",
    "(1) taking the first order Taylor-Series expansion of $G(\\beta)$ and solving for $\\hat{V}(\\hat{\\beta})$. The result is the following: $$\\hat{V}(\\hat{\\beta}) = \\left[I(\\beta)\\right]^{-1} = \\left[-H^{-1}\\hat{V}\\{G(\\hat{\\beta})\\}-H^{-1}\\right]$$ for $\\beta=\\hat{\\beta}$. $H$ is a $k+1 \\times k+1$ matrix of second-derivatives of the log-likelihood $\\left(\\frac{\\partial^2 \\ln L(\\beta|y_j, x_j)}{\\partial \\hat{\\beta} \\partial \\hat{\\beta}'}\\right)$. We refer to this method as the sandwich estimator. \n",
    "\n",
    "(2) The observed information matrix, which is the inverse of the Hessian matrix. This returns the variance-covariance matrix of the paramters and square-rooting the diagonal of variances, returns the standard errors. \n",
    "\n",
    "(3) Outer product of gradients. Taking the inverse of the product of two jacobian matrices returns the variance-covariance matrix for the parameters and likewise, returns the standard errors from squarerooting the diagonal. $$\\left[I(\\beta)\\right]^{-1} = \\left[\\sum^n_{j=1} g_j'g_j\\right]^{-1} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mle_jacobian(params, formulas, data, criterion, design_options):\n",
    "    \"\"\"Jacobian of the pseudo-log-likelihood function for each observation.\n",
    "\n",
    "    Args:\n",
    "        params (pd.DataFrame): The index consists of the parmater names,\n",
    "            the \"value\" column are the parameter values.\n",
    "        formulas (list): a list of strings to be used by patsy to extract\n",
    "            dataframes of the dependent and independent variables\n",
    "        data (pd.DataFrame): a pandas dataset\n",
    "        criterion (string): \"logit\" or \"probit\"\n",
    "        design_options (pd.DataFrame): dataframe containing psu, stratum,\n",
    "            population/design weight and/or a finite population corrector (fpc)\n",
    "\n",
    "    Returns:\n",
    "        jacobian (np.array): an n x k + 1-dimensional array of first\n",
    "            derivatives of the pseudo-log-likelihood function w.r.t. the parameters\n",
    "        \n",
    "    \"\"\"    \n",
    "    y, x = dmatrices(formulas[0], data, return_type='dataframe')\n",
    "    y = y[y.columns[0]] \n",
    "    \n",
    "    if criterion==\"logit\":\n",
    "        c = 1 / (1 + np.exp(-(np.dot(x, params[\"value\"])))) \n",
    "        jacobian = (y - c)[:, None] * x \n",
    "        if \"weight\" in design_options.columns:\n",
    "            weight = design_options[\"weight\"].to_numpy()[:, None]\n",
    "            weighted_jacobian = weight/weight.mean() * jacobian.to_numpy()\n",
    "            return weighted_jacobian\n",
    "        else:\n",
    "            return jacobian.to_numpy()\n",
    "\n",
    "    elif criterion==\"probit\":\n",
    "        pdf = stats.norm._pdf(np.dot(x, params[\"value\"])) \n",
    "        cdf = stats.norm._cdf(np.dot(x, params[\"value\"]))\n",
    "        jacobian = ((pdf / (cdf * (1 - cdf))) * (y - cdf))[:, None] * x\n",
    "        if \"weight\" in design_options.columns:\n",
    "            weight = design_options[\"weight\"].to_numpy()[:, None]\n",
    "            weighted_jacobian = weight/weight.mean() * jacobian.to_numpy()\n",
    "            return weighted_jacobian\n",
    "        else:\n",
    "            return jacobian.to_numpy() \n",
    "    else:\n",
    "        print(\"Criterion function is misspecified or not supported.\")\n",
    "       \n",
    "    \n",
    "def mle_hessian(params, formulas, data, criterion, design_options):\n",
    "    \"\"\"Hessian matrix of the pseudo-log-likelihood function.\n",
    "\n",
    "    Args:\n",
    "        params (pd.DataFrame): The index consists of the parmater names,\n",
    "            the \"value\" column are the parameter values.\n",
    "        formulas (list): a list of strings to be used by patsy to extract\n",
    "            dataframes of the dependent and independent variables\n",
    "        data (pd.DataFrame): a pandas dataset\n",
    "        criterion (string): \"logit\" or \"probit\"\n",
    "        design_options (pd.DataFrame): dataframe containing psu, stratum,\n",
    "            population/design weight and/or a finite population corrector (fpc)\n",
    "    Returns:\n",
    "        hessian (np.array): a k + 1 x k + 1-dimensional array of second derivatives\n",
    "            of the pseudo-log-likelihood function w.r.t. the parameters\n",
    "\n",
    "    \"\"\"    \n",
    "    y, x = dmatrices(formulas[0], data, return_type='dataframe')\n",
    "    y = y[y.columns[0]] \n",
    "    \n",
    "    if criterion==\"logit\":\n",
    "        if \"weight\" in design_options.columns:\n",
    "            weight = design_options[\"weight\"].to_numpy()\n",
    "            c = 1 / (1 + np.exp(-(np.dot(x, params[\"value\"])))) \n",
    "            return -np.dot(weight/weight.mean() * c * (1 - c) * x.T, x)\n",
    "        else:\n",
    "            c = 1 / (1 + np.exp(-(np.dot(x, params[\"value\"])))) \n",
    "            return -np.dot(c * (1 - c) * x.T, x)\n",
    "    elif criterion==\"probit\":\n",
    "        q = 2 * y - 1\n",
    "        pdf = stats.norm._pdf(np.dot(q[:, None] * x, params[\"value\"]))\n",
    "        cdf = stats.norm._cdf(np.dot(q[:, None] * x, params[\"value\"]))\n",
    "        delt = (pdf * q) / cdf\n",
    "        mid = np.dot(x, params[\"value\"]) + delt\n",
    "        if \"weight\" in design_options.columns:\n",
    "            weight = design_options[\"weight\"].to_numpy()\n",
    "            tranpose = (delt * mid)[:, None] * weight[:, None]/weight[:, None].mean() * x\n",
    "            hessian = np.dot(tranpose.T, x)\n",
    "        else:\n",
    "            tranpose = (delt * mid)[:, None] * x\n",
    "            hessian = np.dot(tranpose.T, x)\n",
    "    \n",
    "        return hessian\n",
    "    else:\n",
    "        print(\"Criterion function is misspecified or not supported.\")\n",
    "        \n",
    "def observed_information_matrix(hessian):\n",
    "    oim_var = np.linalg.inv(hessian)\n",
    "    oim_se = np.sqrt(np.diag(oim_var))\n",
    "    return oim_se, oim_var\n",
    "\n",
    "def outer_product_of_gradients(jacobian):\n",
    "    opg_var = np.linalg.inv(np.dot(jacobian.T, jacobian))\n",
    "    opg_se = np.sqrt(np.diag(opg_var))\n",
    "    return opg_se, opg_var\n",
    "\n",
    "def sandwich_step(hessian, meat):\n",
    "    \"\"\"The sandwich estimator for variance estimation.\n",
    "    \n",
    "    Args:\n",
    "        hessian (np.array): 2d numpy array with the hessian of the\n",
    "            pseudo-log-likelihood function evaluated at `params`\n",
    "        meat (np.array): the variance of the total scores\n",
    "        \n",
    "    Returns:\n",
    "        se (np.array): a 1d array of k + 1 standard errors\n",
    "        var (np.array): 2d variance-covariance matrix \n",
    "        \n",
    "    \"\"\"\n",
    "    invhessian = np.linalg.inv(hessian)\n",
    "    var = np.dot(np.dot(invhessian, meat), invhessian)\n",
    "    se = np.sqrt(np.diag(var))\n",
    "    return se, var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inner component, $\\hat{V}\\{G(\\hat{\\beta})\\}$, is the variance of a sum of partial derivatives. The variance of any sum is: $$\\frac{n}{n-1} \\sum^{n}_{j=1}(u_j-\\overline{u_j})^2$$ The mean of the score vector is zero, thus $\\overline{u_j} =0$. Thus, the robust variance estimator is: $$\\hat{V}(\\hat{\\beta}) = \\left[-H^{-1}\\left(\\frac{n}{n-1} \\sum^{n}_{j=1}(u^{'}_ju_j)\\right)-H^{-1}\\right]$$\n",
    "\n",
    "Given its formulation, this is known as a sandwich estimator. The use of this variance estimator is justified for independent observations. In the case where a group of observations are correlated, we have to adjust the estimator to take the sum over clusters compared to taking sums over individual observations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cluster-robust variance is: $$\\hat{V}(\\hat{\\beta}) = \\left[-H^{-1}\\left(\\frac{n_c}{n_c-1} \\sum^{n_c}_{j=1}\\left(\\sum_{j\\in C_i}u_j\\right)^{'}\\left(\\sum_{j\\in C_i}u_j\\right)\\right)-H^{-1}\\right]$$ where $n_c$ refers to the number of clusters and $\\sum_{j\\in C_i}u_j$ is the sum of the scores in cluster $j$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
